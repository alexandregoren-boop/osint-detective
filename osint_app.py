import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import time
from urllib.parse import quote

st.set_page_config("OSINT Detective", layout="wide")
st.title("üîé OSINT Detective ‚Äì Recherche Universelle")

def detect_type(search_input):
    if "@" in search_input:
        return "email"
    elif re.match(r"^\+?\d{7,15}$", search_input.replace(" ", "").replace("-", "")):
        return "t√©l√©phone"
    else:
        return "nom/pr√©nom"

def search_google(query):
    """Recherche Google avec extraction d'informations"""
    results = []
    try:
        # Google Search
        url = f"https://www.google.com/search?q={quote(query)}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extraction des r√©sultats
        for result in soup.find_all('div', class_='g')[:5]:
            title_elem = result.find('h3')
            link_elem = result.find('a')
            snippet_elem = result.find('span', {'data-ved': True})
            
            if title_elem and link_elem:
                title = title_elem.get_text()
                link = link_elem.get('href', '')
                snippet = snippet_elem.get_text() if snippet_elem else ""
                
                # Extraction emails du snippet
                emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', snippet)
                phones = re.findall(r'\b(?:\+33|0)[1-9](?:[0-9]{8})\b', snippet)
                
                results.append({
                    "source": "Google",
                    "titre": title,
                    "lien": link,
                    "extrait": snippet[:200] + "..." if len(snippet) > 200 else snippet,
                    "emails_trouv√©s": ", ".join(emails) if emails else "Aucun",
                    "t√©l√©phones_trouv√©s": ", ".join(phones) if phones else "Aucun"
                })
    except Exception as e:
        results.append({"source": "Google", "erreur": f"Erreur: {str(e)}"})
    
    return results

def search_pages_jaunes(query):
    """Recherche Pages Jaunes"""
    results = []
    try:
        url = f"https://www.pagesjaunes.fr/pagesblanches/recherche?quoiqui={quote(query)}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Recherche des r√©sultats PJ
        for result in soup.find_all('div', class_='bi-denomination')[:3]:
            name = result.get_text().strip()
            parent = result.find_parent('div', class_='bi-bloc')
            
            phone = "Non trouv√©"
            address = "Non trouv√©e"
            
            if parent:
                phone_elem = parent.find('span', class_='coord-numero')
                if phone_elem:
                    phone = phone_elem.get_text().strip()
                
                addr_elem = parent.find('div', class_='adresse')
                if addr_elem:
                    address = addr_elem.get_text().strip()
            
            results.append({
                "source": "Pages Jaunes",
                "nom": name,
                "t√©l√©phone": phone,
                "adresse": address
            })
            
    except Exception as e:
        results.append({"source": "Pages Jaunes", "erreur": f"Erreur: {str(e)}"})
    
    return results

def search_linkedin_google(query):
    """Recherche profils LinkedIn via Google"""
    results = []
    try:
        search_query = f"site:linkedin.com/in {query}"
        url = f"https://www.google.com/search?q={quote(search_query)}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for result in soup.find_all('div', class_='g')[:3]:
            title_elem = result.find('h3')
            link_elem = result.find('a')
            
            if title_elem and link_elem and 'linkedin.com' in link_elem.get('href', ''):
                results.append({
                    "source": "LinkedIn",
                    "profil": title_elem.get_text(),
                    "lien": link_elem.get('href', '')
                })
                
    except Exception as e:
        results.append({"source": "LinkedIn", "erreur": f"Erreur: {str(e)}"})
    
    return results

def search_societe_com(query):
    """Recherche sur soci√©t√©.com"""
    results = []
    try:
        url = f"https://www.societe.com/cgi-bin/search?champs={quote(query)}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extraction des r√©sultats entreprise
        for result in soup.find_all('div', class_='result')[:3]:
            company_name = result.find('h2')
            if company_name:
                results.append({
                    "source": "Soci√©t√©.com",
                    "entreprise": company_name.get_text().strip(),
                    "lien": "https://www.societe.com" + result.find('a').get('href', '')
                })
                
    except Exception as e:
        results.append({"source": "Soci√©t√©.com", "erreur": f"Erreur: {str(e)}"})
    
    return results

# Interface utilisateur
st.sidebar.header("üîç Options de recherche")
search_input = st.sidebar.text_input("Tapez un nom, pr√©nom, email ou num√©ro:", placeholder="Ex: Jean Dupont")

if search_input:
    search_type = detect_type(search_input)
    st.sidebar.info(f"Type d√©tect√©: **{search_type}**")

sources = st.sidebar.multiselect(
    "Sources √† interroger :",
    ["Google", "Pages Jaunes", "LinkedIn", "Soci√©t√©.com"],
    default=["Google", "Pages Jaunes"]
)

# Historique
if 'history' not in st.session_state:
    st.session_state['history'] = []

if 'all_results' not in st.session_state:
    st.session_state['all_results'] = []

# Lancement recherche
if st.button("üöÄ Lancer la recherche") and search_input:
    st.info(f"üîç Recherche de **{search_input}** ({search_type}) sur : {', '.join(sources)}")
    
    # Ajout √† l'historique
    st.session_state['history'].append({
        "recherche": search_input, 
        "type": search_type, 
        "sources": sources,
        "date": time.strftime("%Y-%m-%d %H:%M")
    })
    
    all_results = []
    
    with st.spinner("Recherche en cours..."):
        if "Google" in sources:
            st.write("üîç Recherche Google...")
            google_results = search_google(search_input)
            all_results.extend(google_results)
            
        if "Pages Jaunes" in sources:
            st.write("üìû Recherche Pages Jaunes...")
            pj_results = search_pages_jaunes(search_input)
            all_results.extend(pj_results)
            
        if "LinkedIn" in sources:
            st.write("üíº Recherche LinkedIn...")
            linkedin_results = search_linkedin_google(search_input)
            all_results.extend(linkedin_results)
            
        if "Soci√©t√©.com" in sources:
            st.write("üè¢ Recherche Soci√©t√©.com...")
            societe_results = search_societe_com(search_input)
            all_results.extend(societe_results)
    
    st.session_state['all_results'] = all_results
    
    # Affichage des r√©sultats
    if all_results:
        st.success(f"‚úÖ {len(all_results)} r√©sultats trouv√©s!")
        
        # Grouper par source
        for source in sources:
            source_results = [r for r in all_results if r.get('source') == source]
            if source_results:
                st.subheader(f"üìã R√©sultats {source}")
                df = pd.DataFrame(source_results)
                st.dataframe(df, use_container_width=True)
    else:
        st.warning("Aucun r√©sultat trouv√©.")

# Export
if st.session_state.get('all_results'):
    st.markdown("---")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üìä Exporter en Excel"):
            df = pd.DataFrame(st.session_state['all_results'])
            csv = df.to_csv(index=False)
            st.download_button(
                label="üíæ T√©l√©charger Excel",
                data=csv,
                file_name=f"recherche_{search_input.replace(' ', '_')}.csv",
                mime="text/csv"
            )
    
    with col2:
        if st.button("üìã Voir l'historique"):
            if st.session_state['history']:
                st.dataframe(pd.DataFrame(st.session_state['history']))
            else:
                st.info("Aucune recherche dans l'historique")

st.markdown("---")
st.info("""
üéØ **Cette version recherche de vraies informations :**
- Google : extrait les emails et t√©l√©phones des r√©sultats
- Pages Jaunes : r√©cup√®re noms, t√©l√©phones et adresses  
- LinkedIn : trouve les profils professionnels
- Soci√©t√©.com : trouve les entreprises li√©es

üí° **Prochaines am√©liorations :** Facebook, Instagram, bases d√©c√®s, Google Dorking avanc√©...
""")
