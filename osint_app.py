import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import time
from urllib.parse import quote

st.set_page_config("OSINT Detective", layout="wide")
st.title("üîé OSINT Detective ‚Äì Recherche Universelle")

def detect_type(search_input):
    if "@" in search_input:
        return "email"
    elif re.match(r"^\+?\d{7,15}$", search_input.replace(" ", "").replace("-", "")):
        return "t√©l√©phone"
    else:
        return "nom/pr√©nom"

def get_headers():
    """Headers pour √©viter les blocages"""
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'fr-FR,fr;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
    }

def search_google_advanced(query):
    """Recherche Google avec Google Dorking"""
    results = []
    try:
        # Recherche normale
        searches = [
            f'"{query}"',  # Recherche exacte
            f'{query} email',  # Recherche avec email
            f'{query} t√©l√©phone OR phone',  # Recherche avec t√©l√©phone
            f'{query} site:linkedin.com',  # LinkedIn
            f'{query} site:facebook.com',  # Facebook
            f'{query} site:pagesjaunes.fr OR site:pagesblanches.fr',  # Annuaires
        ]
        
        for search_query in searches[:3]:  # Limite pour √©viter les blocages
            url = f"https://www.google.com/search?q={quote(search_query)}&num=10"
            response = requests.get(url, headers=get_headers())
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extraction am√©lior√©e
            for result in soup.find_all('div', class_='g')[:3]:
                title_elem = result.find('h3')
                link_elem = result.find('a')
                
                if title_elem and link_elem:
                    title = title_elem.get_text()
                    link = link_elem.get('href', '')
                    
                    # Recherche du snippet
                    snippet = ""
                    snippet_elems = result.find_all('span')
                    for elem in snippet_elems:
                        text = elem.get_text()
                        if len(text) > 50:  # Prendre le texte le plus long
                            snippet = text
                            break
                    
                    # Extraction d'informations du snippet
                    emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', snippet + title)
                    phones = re.findall(r'(?:\+33|0)[1-9](?:[.\s-]?\d{2}){4}', snippet + title)
                    
                    results.append({
                        "source": f"Google ({search_query})",
                        "titre": title,
                        "lien": link,
                        "extrait": snippet[:300] + "..." if len(snippet) > 300 else snippet,
                        "emails": ", ".join(set(emails)) if emails else "Aucun",
                        "t√©l√©phones": ", ".join(set(phones)) if phones else "Aucun"
                    })
            
            time.sleep(1)  # Pause pour √©viter les blocages
            
    except Exception as e:
        results.append({"source": "Google", "erreur": f"Erreur: {str(e)}"})
    
    return results

def search_societe_com_advanced(query):
    """Recherche soci√©t√©.com via Google (plus efficace)"""
    results = []
    try:
        # Recherche sur soci√©t√©.com via Google
        search_query = f'site:societe.com "{query}"'
        url = f"https://www.google.com/search?q={quote(search_query)}"
        response = requests.get(url, headers=get_headers())
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for result in soup.find_all('div', class_='g')[:5]:
            title_elem = result.find('h3')
            link_elem = result.find('a')
            snippet_elem = result.find('span', {'data-ved': True})
            
            if title_elem and link_elem and 'societe.com' in link_elem.get('href', ''):
                title = title_elem.get_text()
                link = link_elem.get('href', '')
                snippet = snippet_elem.get_text() if snippet_elem else ""
                
                # Extraction d'informations sp√©cifiques
                siren = re.findall(r'\b\d{9}\b', snippet + title)
                adresses = re.findall(r'\d+[^,]*,\s*\d{5}\s*[A-Z][A-Z\s]+', snippet + title)
                
                results.append({
                    "source": "Soci√©t√©.com",
                    "entreprise": title,
                    "lien": link,
                    "siren": ", ".join(siren) if siren else "Non trouv√©",
                    "adresse": ", ".join(adresses) if adresses else "Non trouv√©e",
                    "extrait": snippet[:200] + "..." if len(snippet) > 200 else snippet
                })
                
    except Exception as e:
        results.append({"source": "Soci√©t√©.com", "erreur": f"Erreur: {str(e)}"})
    
    return results

def search_pages_blanches_advanced(query):
    """Recherche Pages Blanches via Google"""
    results = []
    try:
        search_query = f'site:pagesblanches.fr OR site:pagesjaunes.fr "{query}"'
        url = f"https://www.google.com/search?q={quote(search_query)}"
        response = requests.get(url, headers=get_headers())
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for result in soup.find_all('div', class_='g')[:3]:
            title_elem = result.find('h3')
            link_elem = result.find('a')
            snippet_elem = result.find('span', {'data-ved': True})
            
            if title_elem and link_elem:
                title = title_elem.get_text()
                link = link_elem.get('href', '')
                snippet = snippet_elem.get_text() if snippet_elem else ""
                
                # Extraction t√©l√©phone et adresse
                phones = re.findall(r'(?:\+33|0)[1-9](?:[.\s-]?\d{2}){4}', snippet + title)
                adresses = re.findall(r'\d+[^,]*,\s*\d{5}\s*[A-Z][A-Z\s]+', snippet + title)
                
                results.append({
                    "source": "Pages Blanches/Jaunes",
                    "nom": title,
                    "t√©l√©phone": ", ".join(set(phones)) if phones else "Non trouv√©",
                    "adresse": ", ".join(set(adresses)) if adresses else "Non trouv√©e",
                    "lien": link
                })
                
    except Exception as e:
        results.append({"source": "Pages Blanches", "erreur": f"Erreur: {str(e)}"})
    
    return results

def search_social_networks(query):
    """Recherche sur r√©seaux sociaux via Google"""
    results = []
    try:
        social_sites = [
            "site:linkedin.com/in",
            "site:facebook.com",
            "site:twitter.com OR site:x.com",
            "site:instagram.com"
        ]
        
        for site in social_sites[:2]:  # Limite pour √©viter blocages
            search_query = f'{site} "{query}"'
            url = f"https://www.google.com/search?q={quote(search_query)}"
            response = requests.get(url, headers=get_headers())
            soup = BeautifulSoup(response.text, 'html.parser')
            
            for result in soup.find_all('div', class_='g')[:2]:
                title_elem = result.find('h3')
                link_elem = result.find('a')
                
                if title_elem and link_elem:
                    platform = "LinkedIn" if "linkedin" in link_elem.get('href', '') else \
                              "Facebook" if "facebook" in link_elem.get('href', '') else \
                              "Twitter/X" if ("twitter" in link_elem.get('href', '') or "x.com" in link_elem.get('href', '')) else \
                              "Instagram" if "instagram" in link_elem.get('href', '') else "Autre"
                    
                    results.append({
                        "source": f"R√©seaux sociaux ({platform})",
                        "profil": title_elem.get_text(),
                        "lien": link_elem.get('href', ''),
                        "plateforme": platform
                    })
            
            time.sleep(0.5)
            
    except Exception as e:
        results.append({"source": "R√©seaux sociaux", "erreur": f"Erreur: {str(e)}"})
    
    return results

# Interface utilisateur
st.sidebar.header("üîç Options de recherche")
search_input = st.sidebar.text_input("Tapez un nom, pr√©nom, email ou num√©ro:", placeholder="Ex: Azar Cohen")

if search_input:
    search_type = detect_type(search_input)
    st.sidebar.info(f"Type d√©tect√©: **{search_type}**")

sources = st.sidebar.multiselect(
    "Sources √† interroger :",
    ["Google Avanc√©", "Soci√©t√©.com", "Pages Blanches/Jaunes", "R√©seaux Sociaux"],
    default=["Google Avanc√©", "Soci√©t√©.com"]
)

# Historique
if 'history' not in st.session_state:
    st.session_state['history'] = []

if 'all_results' not in st.session_state:
    st.session_state['all_results'] = []

# Lancement recherche
if st.button("üöÄ Lancer la recherche") and search_input:
    st.info(f"üîç Recherche de **{search_input}** ({search_type}) sur : {', '.join(sources)}")
    
    # Ajout √† l'historique
    st.session_state['history'].append({
        "recherche": search_input, 
        "type": search_type, 
        "sources": sources,
        "date": time.strftime("%Y-%m-%d %H:%M")
    })
    
    all_results = []
    
    with st.spinner("Recherche en cours... Cela peut prendre quelques secondes."):
        if "Google Avanc√©" in sources:
            st.write("üîç Recherche Google avec dorking...")
            google_results = search_google_advanced(search_input)
            all_results.extend(google_results)
            
        if "Soci√©t√©.com" in sources:
            st.write("üè¢ Recherche Soci√©t√©.com...")
            societe_results = search_societe_com_advanced(search_input)
            all_results.extend(societe_results)
            
        if "Pages Blanches/Jaunes" in sources:
            st.write("üìû Recherche Pages Blanches/Jaunes...")
            pages_results = search_pages_blanches_advanced(search_input)
            all_results.extend(pages_results)
            
        if "R√©seaux Sociaux" in sources:
            st.write("üì± Recherche R√©seaux Sociaux...")
            social_results = search_social_networks(search_input)
            all_results.extend(social_results)
    
    st.session_state['all_results'] = all_results
    
    # Affichage des r√©sultats
    if all_results:
        st.success(f"‚úÖ {len(all_results)} r√©sultats trouv√©s!")
        
        # R√©sum√© des informations trouv√©es
        all_emails = []
        all_phones = []
        all_addresses = []
        
        for result in all_results:
            if 'emails' in result and result['emails'] != "Aucun":
                all_emails.extend(result['emails'].split(', '))
            if 't√©l√©phones' in result and result['t√©l√©phones'] != "Aucun":
                all_phones.extend(result['t√©l√©phones'].split(', '))
            if 't√©l√©phone' in result and result['t√©l√©phone'] != "Non trouv√©":
                all_phones.append(result['t√©l√©phone'])
            if 'adresse' in result and result['adresse'] != "Non trouv√©e":
                all_addresses.append(result['adresse'])
        
        # Affichage du r√©sum√©
        if any([all_emails, all_phones, all_addresses]):
            st.subheader("üìã R√©sum√© des informations trouv√©es")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if all_emails:
                    st.write("üìß **Emails:**")
                    for email in set(all_emails):
                        st.write(f"‚Ä¢ {email}")
            
            with col2:
                if all_phones:
                    st.write("üìû **T√©l√©phones:**")
                    for phone in set(all_phones):
                        st.write(f"‚Ä¢ {phone}")
            
            with col3:
                if all_addresses:
                    st.write("üìç **Adresses:**")
                    for address in set(all_addresses):
                        st.write(f"‚Ä¢ {address}")
        
        # Affichage d√©taill√© par source
        st.subheader("üîç R√©sultats d√©taill√©s")
        df = pd.DataFrame(all_results)
        st.dataframe(df, use_container_width=True)
        
    else:
        st.warning("Aucun r√©sultat trouv√©.")

# Export et historique
if st.session_state.get('all_results'):
    st.markdown("---")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üìä Exporter en Excel"):
            df = pd.DataFrame(st.session_state['all_results'])
            csv = df.to_csv(index=False)
            st.download_button(
                label="üíæ T√©l√©charger CSV",
                data=csv,
                file_name=f"recherche_{search_input.replace(' ', '_')}.csv",
                mime="text/csv"
            )
    
    with col2:
        if st.button("üìã Voir l'historique"):
            if st.session_state['history']:
                st.dataframe(pd.DataFrame(st.session_state['history']))
            else:
                st.info("Aucune recherche dans l'historique")

st.markdown("---")
st.info("""
üéØ **Version am√©lior√©e avec recherches plus robustes :**
- Google Dorking avanc√© pour trouver plus d'informations
- Recherche via Google sur soci√©t√©.com, pages blanches, r√©seaux sociaux
- Extraction automatique d'emails, t√©l√©phones, adresses, SIREN
- R√©sum√© des informations de contact trouv√©es
- Recherche sur LinkedIn, Facebook, Twitter/X, Instagram

üí° **Testez avec "Azar Cohen" pour voir les r√©sultats !**
""")
